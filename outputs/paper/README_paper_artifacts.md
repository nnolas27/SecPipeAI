# Paper Artifact README

> Auto-generated by `scripts/generate_manuscript.py`

## Directory Structure

```
outputs/paper/
├── key_numbers.json            — Single source of truth: best models, metrics, CIs
├── final_results_table.csv     — Aggregate results (mean±std over 5 seeds)
├── final_results_table.tex     — LaTeX version of results table
├── final_stats_table.csv       — Statistical tests summary
├── final_stats_table.tex       — LaTeX version of stats table
└── figures/
    ├── roc_comparison_cicids2017.png     — ROC curves, all models, CICIDS2017
    ├── roc_comparison_unsw_nb15.png      — ROC curves, all models, UNSW-NB15
    ├── confusion_best_cicids2017.png     — Confusion matrix, XGBoost, CICIDS2017
    ├── confusion_best_unsw_nb15.png      — Confusion matrix, Random Forest, UNSW-NB15
    ├── macro_f1_bar_cicids2017.png       — Macro-F1 bar chart, CICIDS2017
    └── macro_f1_bar_unsw_nb15.png        — Macro-F1 bar chart, UNSW-NB15
```

## Key Numbers (sourced from pipeline artifacts)

### CICIDS2017
- Best model: XGBoost
- Macro-F1: 0.9981 ± 0.0001 (95% CI: [0.9973, 0.9984])
- PR-AUC: 0.9998 ± 0.0000 (95% CI: [0.9996, 0.9999])
- ROC-AUC: 0.9999
- FAR: 0.09%
- McNemar vs LR: χ² = 41,001, p < 0.0001
- Cliff's δ: 1.0 (large)
- Wilcoxon p (5 seeds): 0.0625

### UNSW-NB15
- Best model: Random Forest
- Macro-F1: 0.8959 ± 0.0004 (95% CI: [0.8917, 0.8975])
- PR-AUC: 0.9935 ± 0.0000 (95% CI: [0.9927, 0.9936])
- ROC-AUC: 0.9862
- FAR: 2.20%
- McNemar vs LR: χ² = 1,761, p < 0.0001
- Cliff's δ: 1.0 (large)
- Wilcoxon p (5 seeds): 0.0625

## Provenance
All numbers are parsed from `outputs/metrics/` produced by the pipeline.
No values were manually fabricated or edited.

## Hardware
- Platform: Linux 6.17.0-14-generic x86_64
- Python: 3.12.3
- CPU-only (no GPU)
- RAM: 7.6 GB
- Total runtime (paper_artifacts target): 47 seconds

## Zenodo Archive
- DOI: [10.5281/zenodo.18766119](https://doi.org/10.5281/zenodo.18766119)
- Archive: https://doi.org/10.5281/zenodo.18766119
