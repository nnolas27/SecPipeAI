"""
Export consolidated metrics as publication-ready CSV, LaTeX, and PNG.

Reads per-model metric JSON files already written by evaluate.py and
aggregates them into:

    outputs/metrics/summary_cicids2017.csv
    outputs/metrics/summary_unsw_nb15.csv
    outputs/metrics/summary.tex          — side-by-side LaTeX table (both datasets)

ROC comparison PNGs (one per dataset) are generated by evaluate.py.
This script reports their locations but does not re-generate them.

Usage:
    python src/evaluation/export.py --config configs/experiment.yaml
    make export
"""

import argparse
from pathlib import Path

import pandas as pd
import yaml

# Metrics columns to include in the published table (in display order)
_TABLE_COLS = ["accuracy", "precision", "recall", "f1", "roc_auc"]

_LATEX_PREAMBLE = r"""\begin{table}[ht]
\centering
\caption{Intrusion Detection Baseline Results}
\label{tab:ids_baseline_results}
\small
"""

_LATEX_POSTAMBLE = r"""\end{table}
"""


def load_metrics_for_dataset(
    metrics_dir: Path,
    dataset: str,
    model_names: list[str],
) -> pd.DataFrame:
    """
    Load outputs/metrics/<dataset>/<model>_metrics.json for each model.
    Returns a DataFrame indexed by model name with columns = _TABLE_COLS.
    Missing metric files emit a warning but do not raise.
    """
    rows = []
    ds_dir = metrics_dir / dataset
    for name in model_names:
        path = ds_dir / f"{name}_metrics.json"
        if not path.exists():
            print(f"  WARNING: metrics file not found — {path} (skipping)")
            continue
        import json
        data = json.loads(path.read_text())
        row = {"model": name}
        for col in _TABLE_COLS:
            row[col] = data.get(col)
        rows.append(row)

    if not rows:
        return pd.DataFrame(columns=["model"] + _TABLE_COLS).set_index("model")
    return pd.DataFrame(rows).set_index("model")


def build_latex_table(dfs: dict[str, pd.DataFrame]) -> str:
    """
    Build a side-by-side LaTeX table with a MultiLevel column header:
        Model | CICIDS2017 [Acc, P, R, F1, AUC] | UNSW-NB15 [Acc, P, R, F1, AUC]

    Uses pd.concat with keys= to create the MultiLevel index.
    """
    combined = pd.concat(dfs.values(), axis=1, keys=list(dfs.keys()))

    latex_inner = combined.to_latex(
        multicolumn=True,
        multicolumn_format="c",
        float_format="%.4f",
        na_rep="—",
        escape=True,
        bold_rows=False,
    )

    return _LATEX_PREAMBLE + latex_inner + _LATEX_POSTAMBLE


def main(config_path: str) -> None:
    with open(config_path) as fh:
        cfg = yaml.safe_load(fh)

    metrics_root = Path(cfg["paths"]["metrics"])
    figures_root = Path(cfg["paths"]["figures"])
    model_names = [m["name"] for m in cfg["models"]]
    datasets = list(cfg["datasets"].keys())

    print("Exporting metrics ...\n")
    dataset_dfs: dict[str, pd.DataFrame] = {}

    for dataset in datasets:
        print(f"  [{dataset}]")
        df = load_metrics_for_dataset(metrics_root, dataset, model_names)

        if df.empty:
            print(f"    No metrics found. Run 'make eval' first.")
            continue

        # Per-dataset CSV
        csv_path = metrics_root / f"summary_{dataset}.csv"
        df.to_csv(csv_path, float_format="%.6f")
        print(f"    CSV  : {csv_path}")

        # Report ROC PNG location
        roc_png = figures_root / dataset / f"roc_comparison_{dataset}.png"
        if roc_png.exists():
            print(f"    ROC  : {roc_png}")
        else:
            print(f"    ROC  : {roc_png}  (not found — run 'make eval' first)")

        dataset_dfs[dataset] = df

    # Combined LaTeX table
    if dataset_dfs:
        tex_path = metrics_root / "summary.tex"
        tex_path.write_text(build_latex_table(dataset_dfs))
        print(f"\n  LaTeX: {tex_path}")
    else:
        print("\n  No data available to build LaTeX table.")

    print("\nExport complete.")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="Export consolidated metrics as CSV + LaTeX + PNG"
    )
    parser.add_argument("--config", default="configs/experiment.yaml")
    args = parser.parse_args()
    main(args.config)
