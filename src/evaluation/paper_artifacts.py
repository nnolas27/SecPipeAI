"""
Orchestrate all paper artifact generation.

Steps:
    1. aggregate.py       — mean±std tables (CSV + LaTeX)
    2. stats_advanced.py  — bootstrap CI, Wilcoxon, Cliff's delta, McNemar
    3. plots_advanced.py  — PR curves, confusion matrix, macro-F1 bar chart
    4. export.py          — consolidated legacy CSV + LaTeX tables
    5. docs generation    — docs/results_summary.md, docs/reproducibility.md

All steps are run via subprocess so each script uses its own process,
keeping memory bounded and avoiding import-path issues.

Writes:
    docs/results_summary.md   — system info, dataset sizes, key numbers, file pointers
    docs/reproducibility.md   — exact versions, pip freeze, seed policy, one-command repro

Usage:
    python src/evaluation/paper_artifacts.py --config configs/experiment.yaml
    make paper_artifacts
"""

import argparse
import json
import platform
import subprocess
import sys
import time
from pathlib import Path

import yaml


# ── System info ───────────────────────────────────────────────────────────────

def get_system_info() -> dict:
    info: dict = {
        "platform": platform.platform(),
        "python": sys.version.split()[0],
        "cpu": platform.processor() or platform.machine(),
        "machine": platform.machine(),
    }
    try:
        with open("/proc/meminfo") as fh:
            for line in fh:
                if line.startswith("MemTotal:"):
                    kb = int(line.split()[1])
                    info["ram_gb"] = round(kb / 1024 / 1024, 1)
                    break
    except Exception:
        info["ram_gb"] = "unknown"
    return info


def get_dataset_sizes(cfg: dict) -> dict:
    import numpy as np
    sizes: dict = {}
    for ds in cfg["datasets"]:
        proc_dir = Path(cfg["paths"]["processed_data"]) / ds
        info: dict = {}
        for split in ["X_train", "X_test", "y_train", "y_test"]:
            p = proc_dir / f"{split}.npy"
            if p.exists():
                arr = np.load(p)
                info[split] = list(arr.shape)
        sizes[ds] = info
    return sizes


def collect_key_numbers(cfg: dict) -> dict:
    """Gather aggregate CSV and stats JSON for the summary doc."""
    import pandas as pd
    numbers: dict = {}
    for ds in cfg["datasets"]:
        agg_csv = Path(cfg["paths"]["metrics"]) / ds / "aggregate" / "summary_mean_std.csv"
        stats_json = Path(cfg["paths"]["metrics"]) / ds / "aggregate" / "stats_advanced.json"
        entry: dict = {}
        if agg_csv.exists():
            df = pd.read_csv(agg_csv, index_col=0)
            entry["aggregate"] = df.to_dict()
        if stats_json.exists():
            entry["stats"] = json.loads(stats_json.read_text())
        numbers[ds] = entry
    return numbers


# ── Subprocess runner ─────────────────────────────────────────────────────────

def run_step(label: str, script: str, config_path: str) -> None:
    print(f"\n--- {label} ---")
    t0 = time.time()
    result = subprocess.run(
        [sys.executable, script, "--config", config_path],
        capture_output=False,
    )
    elapsed = time.time() - t0
    if result.returncode != 0:
        print(f"  WARNING: {label} exited with code {result.returncode} ({elapsed:.1f}s)")
    else:
        print(f"  {label} done ({elapsed:.1f}s)")


# ── Docs generation ───────────────────────────────────────────────────────────

def write_results_summary(
    cfg: dict,
    system_info: dict,
    dataset_sizes: dict,
    key_numbers: dict,
    cmd: str,
    out_path: Path,
    total_elapsed: float,
) -> None:
    lines = [
        "# Results Summary",
        "",
        "> Auto-generated by `make paper_artifacts`. Do not edit manually.",
        "",
        "## System Information",
        "",
        f"- Platform: {system_info.get('platform', 'N/A')}",
        f"- Python:   {system_info.get('python', 'N/A')}",
        f"- CPU:      {system_info.get('cpu', 'N/A')}",
        f"- RAM:      {system_info.get('ram_gb', 'N/A')} GB",
        f"- Computation: CPU-only",
        "",
        "## Exact Command Used",
        "",
        "```bash",
        cmd,
        "```",
        "",
        f"Total paper_artifacts runtime: {total_elapsed:.0f}s",
        "",
        "## Dataset Sizes (after preprocessing)",
        "",
    ]

    for ds, info in dataset_sizes.items():
        lines.append(f"### {ds}")
        for split, shape in info.items():
            lines.append(f"- {split}: {shape}")
        lines.append("")

    lines += [
        "## Key Results",
        "",
        "See `outputs/tables/` for complete CSV/TEX tables.",
        "",
    ]

    for ds, nums in key_numbers.items():
        lines.append(f"### {ds}")
        agg = nums.get("aggregate", {})
        macro_means = agg.get("macro_f1_mean", {})
        macro_stds = agg.get("macro_f1_std", {})
        if macro_means:
            lines += [
                "",
                "**Macro-F1 (mean ± std across seeds):**",
                "",
                "| Model | Mean | Std |",
                "|-------|------|-----|",
            ]
            for model_name, mean_val in macro_means.items():
                std_val = macro_stds.get(model_name, 0.0) or 0.0
                if mean_val is not None:
                    lines.append(f"| {model_name} | {mean_val:.4f} | {std_val:.4f} |")

        stats = nums.get("stats", {})
        if stats:
            best = stats.get("best_model", "")
            lines.append(f"\n**Best model:** `{best}`")

            w = stats.get("wilcoxon", {})
            if "p_value" in w:
                sig = " \\*" if w.get("significant") else ""
                lines.append(
                    f"\n**Wilcoxon** (`{best}` vs `logistic_regression`): "
                    f"p = {w['p_value']:.4f}{sig}"
                )

            cd = stats.get("cliffs_delta", {})
            if "delta" in cd:
                lines.append(
                    f"\n**Cliff's delta**: δ = {cd['delta']:.4f} ({cd.get('magnitude', '')})"
                )

            mcn = stats.get("mcnemar", {})
            if "p_value" in mcn:
                sig = " \\*" if mcn.get("significant") else ""
                lines.append(
                    f"\n**McNemar** (seed={mcn.get('seed_used','')}): "
                    f"chi² = {mcn.get('chi2_stat', 'N/A')}  "
                    f"p = {mcn.get('p_value', 'N/A')}{sig}"
                )

        lines.append("")

    seeds = cfg.get("experiment", {}).get("seeds", [1, 2, 3, 4, 5])
    lines += [
        "## Output File Tree",
        "",
        "```",
        "outputs/",
        "  tables/",
        "    aggregate_cicids2017.{csv,tex}",
        "    aggregate_unsw_nb15.{csv,tex}",
        "    stats_advanced_cicids2017.{csv,tex}",
        "    stats_advanced_unsw_nb15.{csv,tex}",
        "  figures/",
        "    <dataset>/",
        "      roc_comparison_<dataset>.png",
        "      pr_comparison_<dataset>.png",
        "      confusion_best_<dataset>.png",
        "      macro_f1_bar_<dataset>.png",
        "  metrics/",
        "    <dataset>/",
        "      runs/seed_<seed>/<model>_metrics.json",
        "      aggregate/summary_mean_std.{csv,tex}",
        "      aggregate/stats_advanced.{json,csv}",
        "```",
        "",
        f"Seeds used: {seeds}",
    ]

    out_path.parent.mkdir(parents=True, exist_ok=True)
    out_path.write_text("\n".join(lines) + "\n")
    print(f"  Results summary: {out_path}")


def write_reproducibility(cfg: dict, out_path: Path) -> None:
    seeds = cfg.get("experiment", {}).get("seeds", [1, 2, 3, 4, 5])
    seeds_str = " ".join(str(s) for s in seeds)

    try:
        pip_result = subprocess.run(
            [sys.executable, "-m", "pip", "freeze"],
            capture_output=True, text=True, timeout=30,
        )
        pip_freeze = pip_result.stdout.strip()
    except Exception:
        pip_freeze = "# pip freeze failed — run manually: pip freeze"

    lines = [
        "# Reproducibility Guide",
        "",
        "## One-Command Reproduction",
        "",
        "```bash",
        "# 1. Clone and enter the repo",
        "# 2. Download datasets (follow instructions from: make data)",
        "# 3. Run the full pipeline:",
        "make setup",
        "make data",
        "make preprocess_cicids2017",
        "make preprocess_unsw_nb15",
        f"make seeds SEEDS={len(seeds)}",
        "make aggregate",
        "make stats_advanced",
        "make paper_artifacts",
        "```",
        "",
        "## Seed Policy",
        "",
        f"- Default seeds: `{seeds}`",
        "- Seeds control model `random_state` **only** — not data splits.",
        "- CICIDS2017 preprocessing uses a fixed `random_state` from `configs/experiment.yaml`.",
        "- UNSW-NB15 uses the official train/test split (no random split).",
        "- XGBoost uses `tree_method='hist', device='cpu'` for CPU determinism.",
        "- Re-running with the same seeds on the same hardware reproduces identical results.",
        "",
        "## Configuring Seeds",
        "",
        "```bash",
        "# Run with 10 seeds instead of 5:",
        "make seeds SEEDS=10",
        "make aggregate",
        "make stats_advanced",
        "make paper_artifacts",
        "```",
        "",
        "Or edit `configs/experiment.yaml`:",
        "```yaml",
        "experiment:",
        f"  seeds: {seeds}",
        "```",
        "",
        "## Python Version",
        "",
        f"```\n{sys.version}\n```",
        "",
        "## Package Versions",
        "",
        "```",
        pip_freeze,
        "```",
        "",
    ]

    out_path.parent.mkdir(parents=True, exist_ok=True)
    out_path.write_text("\n".join(lines) + "\n")
    print(f"  Reproducibility doc: {out_path}")


# ── Main ──────────────────────────────────────────────────────────────────────

def main(config_path: str) -> None:
    with open(config_path) as fh:
        cfg = yaml.safe_load(fh)

    t_start = time.time()
    print("=== Generating paper artifacts ===")

    run_step("Aggregate",       "src/evaluation/aggregate.py",      config_path)
    run_step("Stats advanced",  "src/evaluation/stats_advanced.py", config_path)
    run_step("Advanced plots",  "src/evaluation/plots_advanced.py", config_path)
    run_step("Export (legacy)", "src/evaluation/export.py",         config_path)

    print("\n--- Generating docs ---")
    t_docs = time.time()
    system_info = get_system_info()
    dataset_sizes = get_dataset_sizes(cfg)
    key_numbers = collect_key_numbers(cfg)
    total_elapsed = time.time() - t_start
    cmd = (
        "make setup && make data && "
        "make preprocess_cicids2017 && make preprocess_unsw_nb15 && "
        f"make seeds SEEDS={len(cfg.get('experiment', {}).get('seeds', [1,2,3,4,5]))} && "
        "make aggregate && make stats_advanced && make paper_artifacts"
    )
    write_results_summary(
        cfg, system_info, dataset_sizes, key_numbers, cmd,
        Path("docs/results_summary.md"), total_elapsed,
    )
    write_reproducibility(cfg, Path("docs/reproducibility.md"))
    print(f"  Docs generated ({time.time() - t_docs:.1f}s)")

    print(f"\nPaper artifacts complete in {total_elapsed:.0f}s")
    print("\nOutput tree:")
    print("  outputs/tables/  — CSV + TEX aggregate and stats tables")
    print("  outputs/figures/ — PNG plots (ROC, PR, confusion, bar chart)")
    print("  docs/            — results_summary.md, reproducibility.md")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Generate all paper artifacts")
    parser.add_argument("--config", default="configs/experiment.yaml")
    args = parser.parse_args()
    main(args.config)
